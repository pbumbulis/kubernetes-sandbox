version: '3'

#all kubernetes components are well dockerized and can be found:
#https://console.cloud.google.com/gcr/images/google-containers/GLOBAL

#let's assume server components are in a dedicated subnet
#and able to reach each other
networks:
  kubenet:
    driver: bridge
    ipam:
      config:
      - subnet: 172.19.2.0/24


services:
  #1-generate certificates. See generate-certs.sh script.
  #contains openssl and kubectl
  #gererates certificates and kubeconfig files
  generate-certs:
    restart: "no"
    build:
      dockerfile: Dockerfile.kubeadm
      context: .
      args:
        K8S_RELEASE: ${K8S_RELEASE}
    entrypoint: /bin/bash
    working_dir: /
    tty: yes
    command: >
      -c '(ls /certs/done) || (./generate-certs.sh && touch /certs/done)'
    environment:
      #where kubectl should keep it's config
      KUBECONFIG: /conf/admin.conf
    networks:
      - kubenet
    volumes:
      - ./generate-certs.sh:/generate-certs.sh
      - ./certs:/certs
      - ./conf:/conf

  #2-deploy etcd. The state of the cluster is saved here.
  kube-etcd1:
    image: gcr.io/etcd-development/etcd:${ETCD_RELEASE}
    deploy:
      restart_policy:
        condition: on-failure
        delay: 0s
        max_attempts: 15
    restart: on-failure
    #set hostname that will be used by api-server to reach etcd
    hostname: kube-etcd1
    entrypoint: /usr/local/bin/etcd
    ports:
      - 2379
    networks:
      - kubenet
    #https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/configuration.md
    command:
      #cluster settings
      - --name=kube-etcd1
      - --initial-advertise-peer-urls=https://kube-etcd1:2380
      - --initial-cluster=kube-etcd1=https://kube-etcd1:2380,kube-etcd2=https://kube-etcd2:2380
      - --initial-cluster-state=new
      - --initial-cluster-token=secret-token
      #listen settings
      - --listen-client-urls=https://0.0.0.0:2379
      - --listen-peer-urls=https://0.0.0.0:2380
      - --advertise-client-urls=https://kube-etcd1:2379
      #tls settings
      - --cert-file=/certs/etcd/server.crt
      - --key-file=/certs/etcd/server.key
      - --client-cert-auth=true
      - --peer-client-cert-auth=true
      - --peer-trusted-ca-file=/certs/etcd/ca-bundle.crt
      - --peer-cert-file=/certs/etcd/peer.crt
      - --peer-key-file=/certs/etcd/peer.key
      #trust kube-apiserver's certificate "apiserver-etcd-client.crt"
      #signed by certs/etcd/ca.crt
      - --trusted-ca-file=/certs/etcd/ca-bundle.crt
    volumes:
      - ./certs:/certs
    # ss command not found in new etcd
    # there is no other tools to make health checks
    #healthcheck:
      #test: 'ss | grep 2379'
      #interval: 3s
      #retries: 3
      #start_period: 3m
    depends_on:
      - generate-certs
  kube-etcd2:
    image: gcr.io/etcd-development/etcd:${ETCD_RELEASE}
    deploy:
      restart_policy:
        condition: on-failure
        delay: 0s
        max_attempts: 15
    restart: on-failure
    #set hostname that will be used by api-server to reach etcd
    hostname: kube-etcd2
    entrypoint: /usr/local/bin/etcd
    ports:
      - 2379
    networks:
      - kubenet
    #https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/configuration.md
    command:
      #cluster settings
      - --name=kube-etcd2
      - --initial-advertise-peer-urls=https://kube-etcd2:2380
      - --initial-cluster=kube-etcd1=https://kube-etcd1:2380,kube-etcd2=https://kube-etcd2:2380
      - --initial-cluster-state=new
      - --initial-cluster-token=secret-token
      #listen settings
      - --listen-client-urls=https://0.0.0.0:2379
      - --listen-peer-urls=https://0.0.0.0:2380
      - --advertise-client-urls=https://kube-etcd2:2379
      #tls settings
      - --cert-file=/certs/etcd/server.crt
      - --key-file=/certs/etcd/server.key
      - --client-cert-auth=true
      - --peer-client-cert-auth=true
      - --peer-trusted-ca-file=/certs/etcd/ca-bundle.crt
      - --peer-cert-file=/certs/etcd/peer.crt
      - --peer-key-file=/certs/etcd/peer.key
      #trust kube-apiserver's certificate "apiserver-etcd-client.crt"
      #signed by certs/etcd/ca.crt
      - --trusted-ca-file=/certs/etcd/ca-bundle.crt
    volumes:
      - ./certs:/certs
    # ss command not found in new etcd
    # there is no other tools to make health checks
    #healthcheck:
      #test: 'ss | grep 2379'
      #interval: 3s
      #retries: 3
      #start_period: 3m
    depends_on:
      - generate-certs

  #3-deploy kube-apiserver.
  #https://kubernetes.io/docs/reference/using-api/api-overview/
  kube-apiserver:
    image: gcr.io/google-containers/kube-apiserver:${K8S_RELEASE}
    deploy:
      restart_policy:
        condition: on-failure
        delay: 0s
        max_attempts: 15
    restart: on-failure
    #kube-apiserver domain name is used by api-server's clients
    #such as kube-controller-manager and scheduler
    hostname: kube-apiserver
    entrypoint: /usr/local/bin/kube-apiserver
    networks:
      - kubenet
    ports:
      - 6443:6443
    command:
      #full list of options
      #https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
      - --anonymous-auth=false
      - --bind-address=0.0.0.0
      - --etcd-servers=https://kube-etcd1:2379

      #for authentication as a etcd's client
      - --etcd-certfile=/certs/apiserver-etcd-client.crt
      - --etcd-keyfile=/certs/apiserver-etcd-client.key

      #trust etcd hosts that use "etcd/server.crt" signed by etcd/ca.crt
      - --etcd-cafile=/certs/etcd/ca-bundle.crt

      #apiserver main certificates
      - --tls-cert-file=/certs/kube-apiserver.crt
      - --tls-private-key-file=/certs/kube-apiserver.key

      #authenticate any client that uses a certificate signed by kubernetes-ca.crt
      - --client-ca-file=/certs/kubernetes-ca-bundle.crt
      - --requestheader-client-ca-file=/certs/front-proxy-ca-bundle.crt

      #enable bootstrap tokens, new feature
      #https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/
      #https://kubernetes.io/docs/reference/access-authn-authz/authentication/#bootstrap-tokens
      #https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/
      - --enable-bootstrap-token-auth

      #https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use
      - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota

      #If enabled, tokens which are deleted from the API will be revoked
      - --service-account-lookup

      #for authentication as a kubelet's client
      - --kubelet-client-certificate=/certs/apiserver-kubelet-client.crt
      - --kubelet-client-key=/certs/apiserver-kubelet-client.key

      #allow creating privileged docker containers
      - --allow-privileged=true

      #for signing bearer tokens of services accounts
      #should be the same as kube-controller-manager's one
      - --service-account-key-file=/certs/kubernetes-ca.key

      #the range of ClusterIPs in the Services
      - --service-cluster-ip-range=10.1.0.0/16

      #enable role based authorization control https://kubernetes.io/docs/reference/access-authn-authz/rbac/#controller-roles
      - --authorization-mode=RBAC

    volumes:
      - ./certs:/certs
    depends_on:
      - kube-etcd1
      - generate-certs

  #4-deploy kubeadm
  #this creates a container with configured kubectl an kubeadm utils
  #besides that it creates temporary kubelet-bootstrap.conf that contains short ttl bootstrap token
  kubeadm:
    build:
      dockerfile: Dockerfile.kubeadm
      context: .
    deploy:
      restart_policy:
        condition: on-failure
        delay: 1s
        max_attempts: 15
    restart: on-failure
    tty: yes
    entrypoint: /bin/bash
    working_dir: /
    #create temporary bootstrap token in apiserver with ttl 1m that should enough for bootstrapping kubelet.
    #then create temporary bootstrap config for kubelet
    #the token will be revoked automatically in 1 minute
    command: >
      -c '
       kubectl --kubeconfig=/conf/admin.conf apply -f /conf/kubelet-tls-bootstrapping.yaml &&
       ((ls /conf/kubelet-bootstrap.conf) ||
       (kubeadm --kubeconfig=/conf/admin.conf token create --ttl 60m k6u9qr.mdacndgrkcs6wamw &&
       kubectl --kubeconfig=/conf/kubelet-bootstrap.conf config set-cluster default-cluster --server=https://kube-apiserver:6443 --certificate-authority certs/kubernetes-ca-bundle.crt --embed-certs &&
       kubectl --kubeconfig=/conf/kubelet-bootstrap.conf config set-credentials kubelet-bootstrap --token=k6u9qr.mdacndgrkcs6wamw &&
       kubectl --kubeconfig=/conf/kubelet-bootstrap.conf config set-context default-system --cluster default-cluster --user kubelet-bootstrap &&
       kubectl --kubeconfig=/conf/kubelet-bootstrap.conf config use-context default-system &&
       chmod 777 /conf && chmod -R 666 /conf/*)) &&
       kubectl --kubeconfig=/conf/admin.conf apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml &&
       helm repo add coredns https://coredns.github.io/helm &&
       helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ &&
       helm repo update &&
       ((helm --kubeconfig=conf/admin.conf list -a -n kube-system | grep coredns) ||
       (helm --kubeconfig=conf/admin.conf upgrade --install --namespace kube-system coredns coredns/coredns --set service.clusterIP=10.1.0.2)) &&
       ((helm --kubeconfig=conf/admin.conf list -a -n kube-system | grep kubernetes-dashboard) ||
       (helm --kubeconfig=/conf/admin.conf upgrade --install --namespace kube-system kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard))'
    environment:
      #where kubeadm and kubectl should keep it's config
      KUBECONFIG: /conf/admin.conf
    volumes:
      - ./conf:/conf
      - ./certs:/certs
    depends_on:
      - generate-certs
      - kube-apiserver
    networks:
      - kubenet


  #5-deploy kube-controller-manager
  #https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/#synopsis
  kube-controller-manager:
    hostname: kube-controller-manager
    image: gcr.io/google-containers/kube-controller-manager:${K8S_RELEASE}
    deploy:
      restart_policy:
        condition: on-failure
        delay: 0s
        max_attempts: 15
    restart: on-failure
    entrypoint: /usr/local/bin/kube-controller-manager
    #full list of options https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
    command:
      #kube config contains kube-controller-manager-apiserver.crt/key
      #for authentication on kube-apiserver
      - --kubeconfig=/conf/kube-controller-manager.conf
      - --authentication-kubeconfig=/conf/kube-controller-manager.conf
      - --authorization-kubeconfig=/conf/kube-controller-manager.conf

      #trust any client that uses a certificate signed by kubernetes-ca.crt
      - --client-ca-file=/certs/kubernetes-ca-bundle.crt
      - --requestheader-client-ca-file=/certs/front-proxy-ca-bundle.crt

      #remove expired tokens, sign bootstrap tokens
      - --controllers=*,tokencleaner,bootstrapsigner

      #use cloud-controller-manager for cloud providers https://kubernetes.io/docs/reference/command-line-tools-reference/cloud-controller-manager/
      - --cloud-provider=external
      #used to issue cluster-scoped certificates
      - --cluster-signing-cert-file=/certs/kubernetes-ca.crt
      - --cluster-signing-key-file=/certs/kubernetes-ca.key

      #this is for tokens generation
      #service-account-private-key-file should be the same as kube-apiserver's one
      - --service-account-private-key-file=/certs/kubernetes-ca.key

      #will be included in service account's token secret
      - --root-ca-file=/certs/kubernetes-ca-bundle.crt

      #each control loop is started using a separate service account
      #https://kubernetes.io/docs/reference/access-authn-authz/rbac/#controller-roles
      - --use-service-account-credentials=true

      #CIDR Range for Pods in cluster.
      - --cluster-cidr=10.244.0.0/16
      #the range of ClusterIPs in the Services
      - --service-cluster-ip-range=10.1.0.0/16
      - --allocate-node-cidrs=true
    volumes:
      - ./conf:/conf
      - ./certs:/certs
    depends_on:
      - kube-apiserver
      - generate-certs
      - kubeadm
    networks:
      - kubenet


  #6-deploy kube-scheduler
  #https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/#synopsis
  kube-scheduler:
    image: gcr.io/google-containers/kube-scheduler:${K8S_RELEASE}
    deploy:
      restart_policy:
        condition: on-failure
        delay: 0s
        max_attempts: 15
    restart: on-failure
    entrypoint: /bin/sh
    hostname: kube-sheduler
    #full list of options https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/
    command: >
      -c 'sleep 10 && /usr/local/bin/kube-scheduler
      --config=/conf/kube-scheduler.yaml
      --authentication-kubeconfig=/conf/kube-scheduler.conf
      --authorization-kubeconfig=/conf/kube-scheduler.conf'
    volumes:
      - ./conf:/conf
    networks:
      - kubenet
    depends_on:
      - kube-apiserver
      - generate-certs
      - kube-controller-manager
      - kubeadm


  #7-deploy node with kubelet and kube-proxy
  kube-node:
    hostname: kube-node
    networks:
      - kubenet
    privileged: yes
    build:
      context: .
      dockerfile: Dockerfile.node
      args:
        K8S_RELEASE: ${K8S_RELEASE}
        CNI_RELEASE: ${CNI_RELEASE}
    deploy:
      restart_policy:
        condition: on-failure
        delay: 1s
        max_attempts: 15
    restart: on-failure
    command: ""
    entrypoint: /node-entrypoint.sh
    depends_on:
      - generate-certs
      - kube-apiserver
      - kubeadm
      - kube-controller-manager
      - kube-scheduler
    healthcheck:
      test: 'curl localhost:10248'
      interval: 1s
      retries: 3
      #start_period: 3m
    volumes:
      - ./conf:/conf
      - ./certs:/certs
